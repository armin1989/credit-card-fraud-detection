{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.manifold import TSNE\nfrom sklearn.manifold import SpectralEmbedding\nfrom mpl_toolkits.mplot3d import Axes3D\n%matplotlib inline\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d32d0587d37ab01e42041e7482a4d35c4ced55b5"},"cell_type":"markdown","source":"# A first look at the data"},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true,"scrolled":true},"cell_type":"code","source":"data = pd.read_csv(\"../input/creditcard.csv\")\ndata.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"26914e84269ee4960530c945bbe92b9298c9f4f9"},"cell_type":"code","source":"data.head(20)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1e28b30b5dbd2a3fbca47da0bbb12d101e78821b"},"cell_type":"code","source":"data.info()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4dcf3174f10e7144e88227d57eec690440132533"},"cell_type":"markdown","source":"Fraud detection problems usually fall under anamoly detection with a huge class impbalance, lets verify this"},{"metadata":{"trusted":true,"_uuid":"01b465d664f623c72da8db8757bd0de693963728"},"cell_type":"code","source":"plt.hist(data[\"Class\"], bins=2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c01ada5b230dc422f7b191a6d418cfaf680eb5a9"},"cell_type":"code","source":"data[\"Class\"].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7b7908fe9e27a11c365ae615faec33cf63ca789e"},"cell_type":"markdown","source":"Observations:\n    - There are 284807 data samples each with 28 obfuscated features + time + amount of transaction (overall 30 features)\n    - We only have numerical features with no missing data\n    - There is a hugh class imbalance problem as it is the case with any outlier/anamoly/fraud detection problem\n    \nIn the rest of this notebook I will under-sample the negative examples (non-fradulent transactions) to be able to treat this as a classification problem. We can also\nconsider anamoly detection but for the sake of EDA we have to somewhat balance the classes."},{"metadata":{"_uuid":"511e7d0e78994065c4a1b324993994056f4bf74b"},"cell_type":"markdown","source":"# Data sub-sampling and cleaning"},{"metadata":{"_uuid":"266eda62498febc0cfeb947f6e846a2cb06be445"},"cell_type":"markdown","source":"Goals :\n    - Balance the classes (almost balanced is also ok)\n    - Look for constant and duplicated features\n    - We already know there are no missing data points so not checking that"},{"metadata":{"_uuid":"dff258eb05e0cc1ff6f895c724756d4b93f33bbe"},"cell_type":"markdown","source":"Separating fradulent and non-fradulent data to be able to under sample negative examples"},{"metadata":{"trusted":true,"_uuid":"1a04e561a3313f4cdba7dd85344cf10b96dfc0df"},"cell_type":"code","source":"#randomly selecting 442 random non-fraudulent transactin, but normalizing data before undersampling\nfraud = data[data['Class'] == 1]\nnon_fraud = data[data['Class'] == 0].sample(len(fraud) * 5)\nnon_fraud.reset_index(drop=True, inplace=True)\nfraud.reset_index(drop=True, inplace=True)\nnew_data = pd.concat([non_fraud, fraud]).sample(frac=1).reset_index(drop=True)\nnew_data.describe()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"650b2f2368c2442eba681129befef17f31c7cd3b"},"cell_type":"markdown","source":"Verifying there are no missing data points:  (this is already apparent from information above but just doing this as practice)"},{"metadata":{"trusted":true,"_uuid":"ee55da2bfdf1b221996e39595eacf5312dc1be59"},"cell_type":"code","source":"null_count = new_data.isnull().sum(axis=0).sort_values(ascending=False)\nnull_count.head(30)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c54b3f9879bf6e4a900d77de614844970216fd3e"},"cell_type":"markdown","source":"Looking for constant features (features taking only one value)"},{"metadata":{"trusted":true,"_uuid":"ea8ff80435b56b95994bdb23f877329fd06d5031"},"cell_type":"code","source":"values_count = new_data.nunique().sort_values()\nnp.sum(values_count == 1)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c69076e42a7984d3d274a7f9afe0fd5172e51356"},"cell_type":"markdown","source":"Ok looks like we don't have any constant features, lets look for duplicates now"},{"metadata":{"trusted":true,"_uuid":"46ad803564cdc8981970177df551d85fb231574d"},"cell_type":"code","source":"duplicates = []\nfor i, ref in enumerate(new_data.columns[:-1]):\n    for other in new_data.columns[i + 1:-1]:\n        if other not in duplicates and np.all(new_data[ref] == new_data[other]):\n            duplicates.append(other)    \nlen(duplicates)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"adf117525ae05f73c69b7411761276539aed0c44"},"cell_type":"markdown","source":"Alright looks like we have a very clean data set! Lets go through the data in the next step."},{"metadata":{"_uuid":"8059f7904e84e5639a7591832976f158ba8d63f5"},"cell_type":"markdown","source":"# Going through data\n  In this part I will :\n      - Find correlation between data and target (to identify more important features)\n      - Find number of highly correlated features (to see if dimensionality reduction such as PCA can be helpful)\n      - Look at distribution of features to see if outlier detection is needed"},{"metadata":{"trusted":true,"_uuid":"add34339cfed904a5fcc7d79e36ac0ec1a9ab834"},"cell_type":"code","source":"corrmat = new_data.corr()\ncorrmat_orig = data.corr()\nf, ax = plt.subplots(figsize=(16, 8))\nplt.subplot(1, 2, 1)\nplt.title('Correlation matrix of sub-sampled data')\nsns.heatmap(corrmat, vmax=1, square=True)\nplt.subplot(1, 2, 2)\nplt.title('Correlation matrix of original data')\nsns.heatmap(corrmat_orig, vmax=1, square=True)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ec4bf996ae2a12d330c9022a6cba2e85b2a2653c"},"cell_type":"markdown","source":"Observations\n- Some features show a very high positive correlation with the class (V2, V4 and V11 for instance)\n- Many more exhibit a large negative correlation (V1, V3, V7, V10, V12, V14, V16-V18)\n- Features themselves might be higlhy correlated. \n- The fact that there is a smaller region in the heatmap that seems to be separate from the rest of it suggest \nthere is some intrinsic organization to the data (clusters of data), features V1-V19 seem to be highly correlated to target\nand to other features in this group.\n- Correlation matrix of original data points is quite different than that of the sub-sampled data. This is because, the sheer number of non-negative examples is so large, the class features \"looks to be\" independent of any feature and is almost always 0. Notice that even in this case some features show strong negative correlation.\n\nLets look at some of these features individually. Starting from distributions of time and amounts since these two are not obfuscated."},{"metadata":{"trusted":true,"_uuid":"09887256319a3b4d200665cfd8010e2bcbbcbd71"},"cell_type":"code","source":"plt.figure(figsize=(16,8))\nplt.subplot(1, 2, 1)\nplt.title('Histogram of Time for non-fraudulent samples')\nsns.distplot(non_fraud[\"Time\"])\nplt.subplot(1, 2, 2)\nplt.title('Histogram of Time for fraudulent samples')\nsns.distplot(fraud[\"Time\"])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b201cddfcc1b68a68679dd0974958af2c2e62184"},"cell_type":"markdown","source":"We can see that time of fraudulent and non-fraudulent transactions are prety much similar,\nbut there are still slight differences so Time can provide some useful information. Lets look at the same for amounts now."},{"metadata":{"trusted":true,"_uuid":"b86efa514aa15dedfb44ee38c0306d2fda8181d0"},"cell_type":"code","source":"plt.figure(figsize=(16,8))\nplt.subplot(1, 2, 1)\nplt.title('Histogram of Time for non-fraudulent samples, mean = %f' % (non_fraud[\"Amount\"].mean()))\nsns.distplot(non_fraud[\"Amount\"])\nplt.subplot(1, 2, 2)\nplt.title('Histogram of Time for fraudulent samples, mean = %f' % (fraud[\"Amount\"].mean()))\nsns.distplot(fraud[\"Amount\"])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2b33bd75cefa1d2b327c21a623f6d96144d273f8"},"cell_type":"markdown","source":"Observations:\n    - Distribution of amounts of transactions for fraudulent samples has a shorter tail \n    - Fraudulent transactions, on average, are larger than non-fraudulent ones, so amount is an important feature!"},{"metadata":{"_uuid":"e0b77f123f8dd700fd9250ca27e19c17c41b01ce"},"cell_type":"markdown","source":"Lets look of features with a high (positive or negative) correlation:"},{"metadata":{"trusted":true,"_uuid":"af8bce84b0fd35c77e887a41fadd3324f8cade6d"},"cell_type":"code","source":"important_feats = new_data.columns[np.abs(corrmat[\"Class\"]) > 0.5]\nimportant_feats","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5ef70a17f910e78a15381913db436d3021a9e7ca"},"cell_type":"markdown","source":"First we will look at distributions of these 12 feature (class is also included but its irrelevant!)"},{"metadata":{"trusted":true,"_uuid":"5c808bb169fb0046abf65b43d59fe94d74b9cb30"},"cell_type":"code","source":"f, ax = plt.subplots(figsize=(24, 32))\nfor i in range(len(important_feats) - 1):\n    plt.subplot(3, 4, i + 1)\n    plt.title(important_feats[i])\n    sns.distplot(new_data[important_feats[i]])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"78198bb638f486fe34804a64689a83049c76bc60"},"cell_type":"markdown","source":"Most of distributions look Gaussian with a one-sided tail so some outlier detection and removal is necessary. Now lets see how Class changes vs these features."},{"metadata":{"trusted":true,"_uuid":"de13212d08e8f7825ecc8adc76143620578760c4"},"cell_type":"code","source":"f, ax = plt.subplots(figsize=(24, 32))\nfor i in range(len(important_feats) - 1):\n    plt.subplot(3, 4, i + 1)\n    plt.title(important_feats[i])\n    sns.boxplot(x='Class', y=important_feats[i], data=new_data)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"af581b32bf345ef09eeb8b745fa13b0934ef84fa"},"cell_type":"markdown","source":"As it is intuitively expected, with these features, examples that correspond to fradulent samples (Class == 1) exhibit a larger dynamic range, we can add this information (i.e., range of 75-th quantile point - 25th quantile point of each feature)  as additional feature to the dataset!\n\nI won't do this here since size of the dataset is small and we can add these features just before we train any models. Just making a note of this! "},{"metadata":{"_uuid":"39bd9c2e2a82ed89605064f792f861e5c3443e63"},"cell_type":"markdown","source":"# Dimensionality reduction and visualization\nIn this section I will perform dimensionality reduction to embed the data in a 2D space, this gives us an idea of how separable the data is, and, if, embedding into a lower dimenison helps.\nI try the following three methods:\n    - Principal component analysis (and kernel PCA)\n    - Indipendent component analysis (ICA)\n    - tSNE\n    - Diffusion mapping (with two different choices for affinity matrix)"},{"metadata":{"trusted":true,"_uuid":"2b7b0dfe2e595bc0968d06ea6858d1968240496c"},"cell_type":"code","source":"from sklearn.manifold import TSNE\nfrom sklearn.manifold import SpectralEmbedding\nfrom sklearn.decomposition import PCA, KernelPCA, FastICA","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"44f91f9118e8f998428ebb503651599144085169"},"cell_type":"markdown","source":"First we can do some mild outlier detection to get rid of irrelevant data"},{"metadata":{"trusted":true,"_uuid":"96cedbd250a3595f22952ab11d1e9d23e21355f7"},"cell_type":"code","source":"lb = new_data.quantile(0.1)\nub = new_data.quantile(0.9)\nrang = ub - lb\nreduced_data = new_data[~((new_data < (lb - 2 * rang)) |(new_data > (ub + 2 * rang))).any(axis=1)]\nfeatures = reduced_data.drop(['Class'], axis=1, inplace=False)\nfeatures = (features - np.mean(features)) / (np.std(features) + 1e-8)\nlabels = reduced_data['Class']","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"70f09bffd202abc1c66fe11209a36228f2738287"},"cell_type":"markdown","source":"**First method : The humble PCA**"},{"metadata":{"trusted":true,"_uuid":"b80dfe8339054eeb1e2fa734d5f7e5c48b715457"},"cell_type":"code","source":"pca_embedding =  PCA(n_components=2) \npca_emb_data = pca_embedding.fit_transform(features.values)\nplt.figure(figsize=(10,10))\nplt.scatter(pca_emb_data[labels == 1, 0], pca_emb_data[labels == 1, 1], color='red', label='positive samples')\nplt.scatter(pca_emb_data[labels == 0, 0], pca_emb_data[labels == 0, 1], color='blue', label='negative samples')\nplt.legend()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9fc857d14ed9109a8c7a37a82f3d5a024ea44491"},"cell_type":"markdown","source":"Very interesting! looks like just applying PCA turns the data into an (almost) linearnly separable data! Lets look at another one"},{"metadata":{"_uuid":"4af85784c9312abe738d4112eecc6f037154f039"},"cell_type":"markdown","source":"**Fist (b) method : Kernel PCA with rbf kernel**"},{"metadata":{"trusted":true,"_uuid":"fced71dfecfa44586ac4463ac8b2054d9b4a47c2"},"cell_type":"code","source":"kpca_embedding =  KernelPCA(n_components=2, kernel='rbf')\nkpca_emb_data = kpca_embedding.fit_transform(features.values)\nplt.figure(figsize=(10,10))\nplt.title('Reduced data with kernel PCA (RBF kernel)')\nplt.scatter(kpca_emb_data[labels == 1, 0], kpca_emb_data[labels == 1, 1], color='red', label='positive samples')\nplt.scatter(kpca_emb_data[labels == 0, 0], kpca_emb_data[labels == 0, 1], color='blue', label='negative samples')\nplt.legend()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b1d1e43bdd4b35b7f1ab8a81da6ff9bf435a74b3"},"cell_type":"markdown","source":"Regular PCA looks better! Lets try ICA:"},{"metadata":{"_uuid":"350669995778a361410d17b3506570d1324e6dfe"},"cell_type":"markdown","source":"**Second method : (Fast) ICA**"},{"metadata":{"trusted":true,"_uuid":"479701a2132c8eb1446b52b1187a2ae9ac9fc99c"},"cell_type":"code","source":"ica_embedding =  FastICA(n_components=2) \nica_emb_data = ica_embedding.fit_transform(features.values)\nplt.figure(figsize=(10,10))\nplt.scatter(ica_emb_data[labels == 1, 0], ica_emb_data[labels == 1, 1], color='red', label='positive samples')\nplt.scatter(ica_emb_data[labels == 0, 0], ica_emb_data[labels == 0, 1], color='blue', label='negative samples')\nplt.legend()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c77bd83717e3e31502f926aec4a7913919fcc018"},"cell_type":"markdown","source":"This is just a rotation of the results with PCA so lets ignor ICA and try tSNE."},{"metadata":{"_uuid":"6c33e3c6908505d120de7a3812abff51a857ce64"},"cell_type":"markdown","source":"**Third method: tSNE**"},{"metadata":{"trusted":true,"_uuid":"3f6c361b392a80160619f7212f00acb812cfe83e"},"cell_type":"code","source":"tsne_embedding =  TSNE(n_components=2) \ntsne_emb_data = tsne_embedding.fit_transform(features.values)\nplt.figure(figsize=(10,10))\nplt.title('Reduced data with tSNE')\nplt.scatter(tsne_emb_data[labels == 1, 0], tsne_emb_data[labels == 1, 1], color='red', label='positive samples')\nplt.scatter(tsne_emb_data[labels == 0, 0], tsne_emb_data[labels == 0, 1], color='blue', label='negative samples')\nplt.legend()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3814380f58541567e38f572876d9d0a26a669725"},"cell_type":"markdown","source":"**Fourth method : diffusion mapping**"},{"metadata":{"trusted":true,"scrolled":true,"_uuid":"d29fae58ebfa8dce70d3142d0b31e2d0b36bfe0a"},"cell_type":"code","source":"spec_embedding = SpectralEmbedding(n_components=2, affinity='rbf')\ntransformed_data2 = spec_embedding.fit_transform(features.values)\nfig = plt.figure(figsize=(8,24))\nplt.subplot(3, 1, 1)\nplt.scatter(transformed_data2[labels == 1, 0], transformed_data2[labels == 1, 1], color='red', label='positive samples')\nplt.legend()\nplt.subplot(3, 1, 2)\nplt.scatter(transformed_data2[labels == 0, 0], transformed_data2[labels == 0, 1], color='blue', label='negative samples')\nplt.legend()\nplt.subplot(3, 1, 3)\nplt.scatter(transformed_data2[labels == 1, 0], transformed_data2[labels == 1, 1], color='red', label='positive samples')\nplt.scatter(transformed_data2[labels == 0, 0], transformed_data2[labels == 0, 1], color='blue', label='negative samples')\nplt.legend()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"503120964b6a730de9e7bca2c4594443d3dd1eee"},"cell_type":"markdown","source":"This looks pretty interesting! it looks like all non-fradulent data is mapped very close to (0, 0), so we may use spectral embedding to augment the given features.\nLets try spectral embedding now with nearest neighbours kernel instead."},{"metadata":{"trusted":true,"_uuid":"f36e6bfee2e66fd9b4b5364b5d6d7fec7260be7d"},"cell_type":"code","source":"spec_embedding2 = SpectralEmbedding(n_components=2, affinity='nearest_neighbors', n_neighbors=30)\ntransformed_data2 = spec_embedding2.fit_transform(features.values)\nfig = plt.figure(figsize=(8,24))\nplt.subplot(3, 1, 1)\nplt.scatter(transformed_data2[labels == 1, 0], transformed_data2[labels == 1, 1], color='red', label='positive samples')\nplt.legend()\nplt.subplot(3, 1, 2)\nplt.scatter(transformed_data2[labels == 0, 0], transformed_data2[labels == 0, 1], color='blue', label='negative samples')\nplt.legend()\nplt.subplot(3, 1, 3)\nplt.scatter(transformed_data2[labels == 1, 0], transformed_data2[labels == 1, 1], color='red', label='positive samples')\nplt.scatter(transformed_data2[labels == 0, 0], transformed_data2[labels == 0, 1], color='blue', label='negative samples')\nplt.legend()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"847631e92853c67e0a133ddc66bcbef61797cee6"},"cell_type":"markdown","source":"In the embeded space, theres quite alot of overlap between fradulent and non-fradulent data but for a very large portion of fradulent points are completely spearable from negative samples! We can use this too! "},{"metadata":{"_uuid":"a7659c2bc7786dd887800b17336f750045f18898"},"cell_type":"markdown","source":"# Conclusions \n\n   -  The dimension of the feature space is not very large\n   -  There are no missing entries, constant or duplicate features, this is a clean data set!\n   -  Dynamic range of features is a good feature itself to be added to existing ones\n   -  Outlier data detection and removal may need to be carried out (not in the first version of the model we build though)\n  -   Data visualisation in 2D space shows that these features may be easily separable if embedded into a different feature space so we have to try this\n\n"},{"metadata":{"_uuid":"474cd8deb8087fc01ab2c3783e4874be605c73e1"},"cell_type":"markdown","source":""}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}